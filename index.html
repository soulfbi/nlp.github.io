<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Java Code</title>
</head>

<body>

    <pre id="policy">
        #

**1.Policy evaluation using Monte Carlo Method (using any method for 4×4 grid world)**


import numpy as np
from collections import defaultdict
np.random.seed(0)

# -------- GRID SIZE --------
rows = 4
cols = 4
n = rows * cols

# terminal states
terminal = [0, n-1]

# actions = L, R, U, D
actions = [(0,-1),(0,1),(-1,0),(1,0)]

def step(s, a):
    r, c = divmod(s, cols)
    dr, dc = a
    nr, nc = r + dr, c + dc

    # invalid move → stay in same state
    if nr < 0 or nr >= rows or nc < 0 or nc >= cols:
        nr, nc = r, c

    ns = nr * cols + nc
    return ns, -1      # reward = -1


# -------- RANDOM POLICY (π(a|s) = 1/4) --------
def policy(s):
    return actions[np.random.randint(4)]


# -------- MONTE CARLO POLICY EVALUATION --------
V = defaultdict(float)
returns = defaultdict(list)

episodes = 5000

for _ in range(episodes):

    # start from a random NON-terminal state
    s = np.random.randint(1, n-1)
    while s in terminal:
        s = np.random.randint(1, n-1)

    episode = []

    # generate full episode
    while True:
        a = policy(s)
        ns, r = step(s, a)
        episode.append((s, r))

        if ns in terminal:
            break
        s = ns

    # FIRST-VISIT MC UPDATE
    G = 0
    visited = set()

    for s, r in reversed(episode):
        G += r
        if s not in visited:
            visited.add(s)
            returns[s].append(G)
            V[s] = np.mean(returns[s])


# -------- PRINT VALUE FUNCTION --------
value_grid = np.zeros((rows, cols))
for s, v in V.items():
    value_grid[s // cols][s % cols] = round(v, 2)

print("Monte Carlo Value Function (4x4 Grid):\n")
print(value_grid)


    </pre>

    <pre id="mdp">
        #Q2: Simple MDP simulation in a 4×4 grid world for Markov Decision Process (MDP) for planning with Value Iteration
import numpy as np

GRID_SIZE = 4
STATE_COUNT = GRID_SIZE * GRID_SIZE
TERMINAL_STATES = [0, STATE_COUNT - 1]
ACTIONS = ["U", "D", "L", "R"]
GAMMA = 1.0
THRESHOLD = 1e-4

V = np.zeros(STATE_COUNT)

def state_to_pos(state):
    return state // GRID_SIZE, state % GRID_SIZE

def pos_to_state(row, col):
    return row * GRID_SIZE + col

def step(state, action):
    if state in TERMINAL_STATES:
        return state, 0

    r, c = state_to_pos(state)

    if action == "U": r = max(r - 1, 0)
    elif action == "D": r = min(r + 1, GRID_SIZE - 1)
    elif action == "L": c = max(c - 1, 0)
    elif action == "R": c = min(c + 1, GRID_SIZE - 1)

    next_state = pos_to_state(r, c)
    reward = -1
    return next_state, reward

iteration = 0
while True:
    delta = 0
    new_V = np.copy(V)

    for s in range(STATE_COUNT):
        if s in TERMINAL_STATES:
            continue

        values = []
        for action in ACTIONS:
            next_state, reward = step(s, action)
            values.append(reward + GAMMA * V[next_state])

        new_V[s] = max(values)
        delta = max(delta, abs(V[s] - new_V[s]))

    V = new_V
    iteration += 1

    if delta < THRESHOLD:
        break

print("Converged in", iteration, "iterations")
print("\nOptimal Value Function:")
print(np.round(V.reshape(GRID_SIZE, GRID_SIZE), 2))

policy = {}
for s in range(STATE_COUNT):
    if s in TERMINAL_STATES:
        policy[s] = None
    else:
        q_values = []
        for action in ACTIONS:
            next_state, reward = step(s, action)
            q_values.append(reward + GAMMA * V[next_state])
        best_action = ACTIONS[np.argmax(q_values)]
        policy[s] = best_action

print("\nOptimal Policy (as arrows):")
policy_symbols = {"U": "↑", "D": "↓", "L": "←", "R": "→", None: "T"}

output = []
for s in range(STATE_COUNT):
    output.append(policy_symbols[policy[s]])
output = np.array(output).reshape(GRID_SIZE, GRID_SIZE)
print(output)


    </pre>

    <pre id="mdprollout">
        # Simple MDP simulation in a 4x4 grid world for markov decision process (MDP) for Rollout the optimal policy from (0,0)

import numpy as np

# Gridworld parameters
N_ROWS = 4
N_COLS = 4
N_STATES = N_ROWS * N_COLS
ACTIONS = ['U', 'D', 'L', 'R']
ACTION_DELTAS = {
    'U': (-1, 0),
    'D': (1, 0),
    'L': (0, -1),
    'R': (0, 1),
}
TERMINAL_STATES = [0, 15]

ARROWS = {
    'U': '↑',
    'D': '↓',
    'L': '←',
    'R': '→',
    'T': 'T'
}

def state_to_pos(s):
    return divmod(s, N_COLS)

def pos_to_state(r, c):
    return r * N_COLS + c

def step(state, action):
    """Returns next_state, reward."""
    if state in TERMINAL_STATES:
        return state, 0

    r, c = state_to_pos(state)
    dr, dc = ACTION_DELTAS[action]
    nr = min(max(r + dr, 0), N_ROWS - 1)
    nc = min(max(c + dc, 0), N_COLS - 1)
    next_state = pos_to_state(nr, nc)

    reward = -1
    return next_state, reward

def value_iteration(gamma=1.0, theta=1e-4):
    """Runs Value Iteration and returns optimal V and optimal policy."""
    V = np.zeros(N_STATES)

    while True:
        delta = 0
        new_V = np.copy(V)

        for s in range(N_STATES):
            if s in TERMINAL_STATES:
                continue

            q_values = []
            for a in ACTIONS:
                ns, r = step(s, a)
                q_values.append(r + gamma * V[ns])

            new_V[s] = max(q_values)
            delta = max(delta, abs(new_V[s] - V[s]))

        V = new_V
        if delta < theta:
            break

    # Extract optimal policy
    policy = np.empty(N_STATES, dtype=str)
    for s in range(N_STATES):
        if s in TERMINAL_STATES:
            policy[s] = 'T'
            continue

        q_values = []
        for a in ACTIONS:
            ns, r = step(s, a)
            q_values.append((r + gamma * V[ns], a))

        policy[s] = max(q_values)[1]

    return V, policy

def rollout_policy(policy, start_state=0):
    """Simulates following the optimal policy from start_state."""
    state = start_state
    trajectory = []

    while True:
        trajectory.append(state)
        if state in TERMINAL_STATES:
            break
        action = policy[state]
        next_state, reward = step(state, action)
        state = next_state

    return trajectory

def print_grid(values):
    for r in range(N_ROWS):
        row = values[r*N_COLS:(r+1)*N_COLS]
        print(" ".join(f"{x:6.2f}" if isinstance(x, (int,float)) else f"  {x}  " for x in row))
    print()

def print_policy_grid(policy):
    arrow_policy = [ARROWS[p] for p in policy]
    for r in range(N_ROWS):
        row = arrow_policy[r*N_COLS:(r+1)*N_COLS]
        print(" ".join(f"  {x}  " for x in row))
    print()

if __name__ == "__main__":
    # Step 1: Compute optimal V and policy
    V, policy = value_iteration()

    print("\nOptimal Value Function:")
    print_grid(V)

    print("Optimal Policy (arrows):")
    print_policy_grid(policy)

    # Step 2: Rollout from (0,0)
    print("Rollout from start (0,0):")
    trajectory = rollout_policy(policy, start_state=0)
    print(" → ".join(str(s) for s in trajectory))

    </pre>

    <pre id="mdpunknown">
        # Simple MDP simulation in a 4x4 grid world for Markov Decision Process (MDP)
# Learn from interaction with Q-learning (unknown model)

import numpy as np
import random

# Gridworld parameters
N_ROWS = 4
N_COLS = 4
N_STATES = N_ROWS * N_COLS
ACTIONS = ['U', 'D', 'L', 'R']
ACTION_DELTAS = {
    'U': (-1, 0),
    'D': (1, 0),
    'L': (0, -1),
    'R': (0, 1),
}
TERMINAL_STATES = [0, 15]

# Arrow symbols
ARROWS = {
    'U': '↑',
    'D': '↓',
    'L': '←',
    'R': '→',
    'T': 'T'
}

# --- Helpers ---
def pos_to_state(r, c):
    return r * N_COLS + c

def state_to_pos(s):
    return divmod(s, N_COLS)

def step(state, action):
    """Environment transition: returns next_state, reward, done."""
    if state in TERMINAL_STATES:
        return state, 0, True

    r, c = state_to_pos(state)
    dr, dc = ACTION_DELTAS[action]

    nr = min(max(r + dr, 0), N_ROWS - 1)
    nc = min(max(c + dc, 0), N_COLS - 1)
    next_state = pos_to_state(nr, nc)

    reward = -1
    done = next_state in TERMINAL_STATES
    return next_state, reward, done

# --- Q-learning ---
def q_learning(alpha=0.1, gamma=0.99, epsilon=0.1, episodes=5000, max_steps=100):
    Q = np.zeros((N_STATES, len(ACTIONS)))

    for ep in range(episodes):
        state = random.choice([s for s in range(N_STATES) if s not in TERMINAL_STATES])

        for _ in range(max_steps):
            # ε-greedy selection
            if random.random() < epsilon:
                action_idx = random.randint(0, len(ACTIONS) - 1)
            else:
                action_idx = np.argmax(Q[state])

            action = ACTIONS[action_idx]
            next_state, reward, done = step(state, action)

            # Q-learning update
            td_target = reward + gamma * np.max(Q[next_state])
            Q[state, action_idx] += alpha * (td_target - Q[state, action_idx])

            state = next_state
            if done:
                break

    return Q

def extract_policy(Q):
    """Greedy policy using arrow symbols."""
    policy = []
    for s in range(N_STATES):
        if s in TERMINAL_STATES:
            policy.append(ARROWS['T'])
        else:
            best_a = ACTIONS[np.argmax(Q[s])]
            policy.append(ARROWS[best_a])
    return policy

def rollout(policy, start_state):
    """Rollout the learned policy."""
    state = start_state
    traj = [state]

    while state not in TERMINAL_STATES:
        # Convert arrow back to action letter
        arrow_to_action = {v: k for k, v in ARROWS.items()}
        action = arrow_to_action[policy[state]]

        next_state, reward, done = step(state, action)
        traj.append(next_state)
        state = next_state
        if done:
            break
    return traj

def print_grid(values):
    for r in range(N_ROWS):
        row = values[r*N_COLS:(r+1)*N_COLS]
        print(" ".join(f"  {v}  " for v in row))
    print()

# --- Run training ---
if __name__ == "__main__":
    Q = q_learning(episodes=8000)
    policy = extract_policy(Q)

    print("\nLearned Q-values:")
    for s in range(N_STATES):
        print(f"State {s}: {Q[s]}")

    print("\nLearned Policy (arrows):")
    print_grid(policy)

    print("Rollout from start state (0,1):")
    print(rollout(policy, start_state=1))

    </pre>

    <pre id="td0">

#TD(0) Learning

import numpy as np
import random

# -----------------------------
# Simple Random Walk Environment
# -----------------------------
class RandomWalkEnv:
    """
    States: 0 1 2 3 4 5 6
    0 and 6 are terminal states.
    Start state = 3 every episode.
    Reward = +1 only when reaching state 6, else 0.
    """
    def __init__(self):
        self.start_state = 3
        self.terminal_states = [0, 6]
        self.state = self.start_state

    def reset(self):
        """Reset environment to start of episode."""
        self.state = self.start_state
        return self.state

    def step(self, action):
        """
        action: -1 = move left, +1 = move right
        returns: next_state, reward, done
        """
        next_state = self.state + action

        # Reward only if right terminal (state 6)
        if next_state == 6:
            reward = 1.0
        else:
            reward = 0.0

        self.state = next_state
        done = next_state in self.terminal_states
        return next_state, reward, done


# -----------------------------
# TD(0) Algorithm (Prediction)
# -----------------------------
def td0_prediction(num_episodes=100, alpha=0.1, gamma=1.0):
    """
    Temporal Difference (TD(0)) Learning to estimate V(s).

    num_episodes: how many episodes to run
    alpha: learning rate
    gamma: discount factor
    """
    env = RandomWalkEnv()

    # There are 7 states: 0 to 6
    # Initialize value function V(s)
    V = np.zeros(7)

    # Terminal states are fixed (by definition of the problem)
    V[0] = 0.0
    V[6] = 0.0

    for episode in range(num_episodes):
        state = env.reset()

        while True:
            # Policy: choose left or right with equal probability
            action = random.choice([-1, 1])

            next_state, reward, done = env.step(action)

            # TD(0) Update:
            # V(s) ← V(s) + α [ r + γ V(s') - V(s) ]
            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]
            V[state] = V[state] + alpha * td_error

            state = next_state

            if done:
                break

    return V


if __name__ == "__main__":
    # Run TD(0)
    num_episodes = 200  # you can increase for better estimates
    alpha = 0.1
    gamma = 1.0

    V = td0_prediction(num_episodes=num_episodes, alpha=alpha, gamma=gamma)

    print("Estimated State Values after TD(0):")
    for s in range(7):
        print(f"V({s}) = {V[s]:.3f}")

    </pre>

    <pre id="td1">

#TD(lambda)Learning
import numpy as np
import random

# -----------------------------
# Simple Random Walk Environment
# -----------------------------
class RandomWalkEnv:
    """
    States: 0 1 2 3 4 5 6
    0 and 6 are terminal states.
    Start state = 3 every episode.
    Reward = +1 only when reaching state 6, else 0.
    """
    def __init__(self):
        self.start_state = 3
        self.terminal_states = [0, 6]
        self.state = self.start_state

    def reset(self):
        """Reset environment to start of episode."""
        self.state = self.start_state
        return self.state

    def step(self, action):
        """
        action: -1 = move left, +1 = move right
        returns: next_state, reward, done
        """
        next_state = self.state + action

        # Reward only if right terminal (state 6)
        if next_state == 6:
            reward = 1.0
        else:
            reward = 0.0

        self.state = next_state
        done = next_state in self.terminal_states
        return next_state, reward, done


# -----------------------------
# TD(λ) Algorithm (Prediction)
# -----------------------------
def td_lambda_prediction(num_episodes=100, alpha=0.1, gamma=1.0, lam=0.8):
    """
    Temporal Difference TD(λ) Learning to estimate V(s) using eligibility traces.

    num_episodes: number of episodes
    alpha       : learning rate
    gamma       : discount factor
    lam         : lambda parameter (0 <= λ <= 1)
    """
    env = RandomWalkEnv()

    # There are 7 states: 0 to 6
    V = np.zeros(7)

    # terminal values
    V[0] = 0.0
    V[6] = 0.0

    for episode in range(num_episodes):
        state = env.reset()

        # Initialize eligibility traces to 0 for all states
        E = np.zeros(7)

        while True:
            # Behaviour policy: random left/right
            action = random.choice([-1, 1])

            next_state, reward, done = env.step(action)

            # TD error δ = r + γ V(s') − V(s)
            td_target = reward + gamma * V[next_state]
            td_error = td_target - V[state]

            # Increase eligibility of current state
            E[state] += 1.0     # accumulating traces

            # Update all states' values using their eligibility
            V += alpha * td_error * E

            # Decay eligibility traces
            E *= gamma * lam

            state = next_state

            if done:
                break

    return V


if __name__ == "__main__":
    num_episodes = 200
    alpha = 0.1
    gamma = 1.0
    lam = 0.8

    V = td_lambda_prediction(num_episodes=num_episodes,
                             alpha=alpha, gamma=gamma, lam=lam)

    print("Estimated State Values after TD(λ):")
    for s in range(7):
        print(f"V({s}) = {V[s]:.3f}")

    </pre>

    <pre id="sarsa">
        #**7.Implement SARSA**
import numpy as np
np.random.seed(3)

terminal=[0,15]
actions=[-1,1,-4,4]

def step(s,a):
    ns=s+a
    if s%4==0 and a==-1: ns=s
    if s%4==3 and a==1:  ns=s
    if s<4 and a==-4:    ns=s
    if s>11 and a==4:    ns=s
    return ns, -1

Q = np.zeros((16,4))
alpha=0.1; gamma=0.9; eps=0.1

def choose_action(s):
    return np.random.randint(4) if np.random.rand()<eps else np.argmax(Q[s])

for ep in range(5000):
    s=np.random.randint(1,15)
    a=choose_action(s)
    while s not in terminal:
        ns,r=step(s, actions[a])
        na=choose_action(ns)
        Q[s,a]+=alpha*(r + gamma*Q[ns,na] - Q[s,a])
        s, a = ns, na

print("Q-values:\n",Q)
print("\nOptimal Policy (0=L,1=R,2=U,3=D):")
print(np.argmax(Q,axis=1).reshape(4,4))

    </pre>

    <pre id="qlearning">
        # **8.Implement Q Learning**
import numpy as np
np.random.seed(5)

terminal=[0,15]
actions=[-1,1,-4,4]

def step(s,a):
    ns=s+a
    if s%4==0 and a==-1: ns=s
    if s%4==3 and a==1:  ns=s
    if s<4 and a==-4:    ns=s
    if s>11 and a==4:    ns=s
    return ns, -1

Q=np.zeros((16,4))
alpha=0.1; gamma=0.9; eps=0.1

def choose(s):
    return np.random.randint(4) if np.random.rand()<eps else np.argmax(Q[s])

for ep in range(5000):
    s=np.random.randint(1,15)
    while s not in terminal:
        a=choose(s)
        ns,r=step(s, actions[a])
        Q[s,a]+=alpha*(r + gamma*np.max(Q[ns]) - Q[s,a])
        s=ns

print("Q-table:\n",Q)
print("\nOptimal Policy:")
print(np.argmax(Q,axis=1).reshape(4,4))

    </pre>

    <pre id="dqn">
        #DQN
import random, collections, math, numpy as np, gymnasium as gym, torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Simple Q-Network
class QNetwork(nn.Module):
    def __init__(self, obs_dim, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, n_actions)
        )
    def forward(self, x):
        return self.net(x)

# Replay buffer
Transition = collections.namedtuple("Transition", ("s", "a", "r", "s2", "done"))
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = collections.deque(maxlen=capacity)
    def push(self, *args): self.buffer.append(Transition(*args))
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        return Transition(*zip(*batch))
    def __len__(self): return len(self.buffer)

# DQN training function
def train_dqn():
    env = gym.make("CartPole-v1")
    obs_dim = env.observation_space.shape[0]
    n_actions = env.action_space.n

    policy_net = QNetwork(obs_dim, n_actions).to(device)
    target_net = QNetwork(obs_dim, n_actions).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)
    buffer = ReplayBuffer()
    GAMMA, BATCH_SIZE, TARGET_UPDATE, EPS_DECAY = 0.99, 64, 500, 5000
    EPS_START, EPS_END = 1.0, 0.01
    total_steps, rewards = 0, []

    def epsilon(step):
        return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * step / EPS_DECAY)

    for ep in range(300):
        state, _ = env.reset()
        done, ep_reward = False, 0
        while not done:
            eps = epsilon(total_steps)
            total_steps += 1
            if random.random() < eps:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    s = torch.tensor(np.array(state), dtype=torch.float32, device=device).unsqueeze(0)
                    action = int(policy_net(s).argmax(1).item())

            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            buffer.push(state, action, reward, next_state, done)
            state = next_state
            ep_reward += reward

            if len(buffer) >= BATCH_SIZE:
                transitions = buffer.sample(BATCH_SIZE)
                s = torch.tensor(np.array(transitions.s), dtype=torch.float32, device=device)
                a = torch.tensor(transitions.a, dtype=torch.int64, device=device).unsqueeze(1)
                r = torch.tensor(transitions.r, dtype=torch.float32, device=device).unsqueeze(1)
                s2 = torch.tensor(np.array(transitions.s2), dtype=torch.float32, device=device)
                done_mask = torch.tensor(transitions.done, dtype=torch.float32, device=device).unsqueeze(1)

                q_vals = policy_net(s).gather(1, a)
                with torch.no_grad():
                    q_next = target_net(s2).max(1)[0].unsqueeze(1)
                    q_target = r + GAMMA * q_next * (1 - done_mask)

                loss = nn.functional.mse_loss(q_vals, q_target)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            if total_steps % TARGET_UPDATE == 0:
                target_net.load_state_dict(policy_net.state_dict())

        rewards.append(ep_reward)
        if ep % 10 == 0:
            print(f"Episode {ep} | Avg Reward: {np.mean(rewards[-10:]):.2f}")

    env.close()
    plt.plot(rewards)
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("DQN Training (Gymnasium - CartPole)")
    plt.show()

train_dqn()

    </pre>

    <pre id="reinforce">
        # Implement REINFORCE algorithm with Baseline

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np

# ---------------------------------------------
#  Policy Network (Actor)
# ---------------------------------------------
class PolicyNetwork(nn.Module):
    def _init_(self, state_dim, action_dim):
        super()._init_()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.fc(x)

# ---------------------------------------------
#  Baseline Network (State Value)
# ---------------------------------------------
class ValueNetwork(nn.Module):
    def _init_(self, state_dim):
        super()._init_()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.fc(x)

# ---------------------------------------------
#  Compute discounted returns
# ---------------------------------------------
def compute_returns(rewards, gamma=0.99):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, dtype=torch.float32)

# ---------------------------------------------
#  REINFORCE with Baseline – Training Loop
# ---------------------------------------------
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

policy_net = PolicyNetwork(state_dim, action_dim)
value_net = ValueNetwork(state_dim)

policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)
value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)

num_episodes = 100
gamma = 0.99

for episode in range(num_episodes):
    state, _ = env.reset()
    log_probs, values, rewards = [], [], []
    done = False

    while not done:
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = policy_net(state_tensor)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()

        next_state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        log_probs.append(dist.log_prob(action))
        values.append(value_net(state_tensor))
        rewards.append(reward)

        state = next_state

    # Compute returns
    returns = compute_returns(rewards, gamma)
    values = torch.cat(values).squeeze()

    # Baseline is value function (detached)
    baseline = values.detach()

    # Advantage = Return - Baseline
    advantages = returns - baseline

    # Policy loss
    policy_loss = -(torch.stack(log_probs) * advantages).mean()

    # Value loss = MSE(Returns, Baseline)
    value_loss = (returns - values).pow(2).mean()

    # Update policy
    policy_optimizer.zero_grad()
    policy_loss.backward()
    policy_optimizer.step()

    # Update value baseline
    value_optimizer.zero_grad()
    value_loss.backward()
    value_optimizer.step()

    print(f"Episode {episode+1}: Reward = {sum(rewards)}")

env.close()
print("REINFORCE with baseline training finished.")
    </pre>

    <pre id="reinforce1">
        # Implement REINFORCE algorithm with Advantage Function.

import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np

# ---------------------------------------------
#  Policy Network (Actor)
# ---------------------------------------------
class PolicyNetwork(nn.Module):
    def _init_(self, state_dim, action_dim):
        super()._init_()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.fc(x)

# ---------------------------------------------
#  Value Network (Critic)
# ---------------------------------------------
class ValueNetwork(nn.Module):
    def _init_(self, state_dim):
        super()._init_()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.fc(x)

# ---------------------------------------------
#  Compute discounted returns
# ---------------------------------------------
def compute_returns(rewards, gamma=0.99):
    R = 0
    returns = []
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, dtype=torch.float32)

# ---------------------------------------------
#  REINFORCE with Advantage – Training Loop
# ---------------------------------------------
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

policy_net = PolicyNetwork(state_dim, action_dim)
value_net = ValueNetwork(state_dim)

policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)
value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)

num_episodes = 100
gamma = 0.99

for episode in range(num_episodes):
    state, _ = env.reset()
    log_probs, values, rewards = [], [], []
    done = False

    while not done:
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = policy_net(state_tensor)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()

        next_state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        log_probs.append(dist.log_prob(action))
        values.append(value_net(state_tensor))
        rewards.append(reward)

        state = next_state

    # Compute returns
    returns = compute_returns(rewards, gamma)
    values = torch.cat(values).squeeze()

    # ADVANTAGE = Return − Value Estimate
    advantages = returns - values.detach()

    # POLICY LOSS (Actor)
    policy_loss = -(torch.stack(log_probs) * advantages).mean()

    # VALUE LOSS (Critic)
    value_loss = (returns - values).pow(2).mean()

    # Update policy
    policy_optimizer.zero_grad()
    policy_loss.backward()
    policy_optimizer.step()

    # Update critic
    value_optimizer.zero_grad()
    value_loss.backward()
    value_optimizer.step()

    print(f"Episode {episode+1}: Reward = {sum(rewards)}")

env.close()
print("REINFORCE with advantage training finished.")
    </pre>

    <pre id="control">
        # **12. Implement the Monte Carlo prediction and control.**
import numpy as np
np.random.seed(0)

actions = [-1, 1, -4, 4]         # L, R, U, D
terminal = [0, 15]

# Arrow mapping
ARROWS = {
    -1: "←",
     1: "→",
    -4: "↑",
     4: "↓",
    "T": "T"
}

def step(s,a):
    ns = s + a
    if s % 4 == 0 and a == -1: ns = s
    if s % 4 == 3 and a == 1:  ns = s
    if s < 4 and a == -4:      ns = s
    if s > 11 and a == 4:      ns = s
    return ns, -1

Q = np.zeros((16,4))
returns = { (s,a):[] for s in range(16) for a in range(4) }
eps = 0.1
gamma = 1.0

def choose(s):   # ε-greedy
    if np.random.rand() < eps:
        return np.random.randint(4)
    return np.argmax(Q[s])

# Monte Carlo Control
for ep in range(5000):
    s = np.random.randint(1,15)
    episode = []

    while s not in terminal:
        a = choose(s)
        ns, r = step(s, actions[a])
        episode.append((s,a,r))
        s = ns

    G = 0
    for i in reversed(range(len(episode))):
        s, a, r = episode[i]
        G += r

        # First-visit check
        if not any(s == x[0] and a == x[1] for x in episode[:i]):
            returns[(s,a)].append(G)
            Q[s,a] = np.mean(returns[(s,a)])

# Greedy policy
policy_idx = np.argmax(Q, axis=1)

# Convert to arrows
arrow_policy = []
for s in range(16):
    if s in terminal:
        arrow_policy.append(ARROWS["T"])
    else:
        arr = ARROWS[ actions[ policy_idx[s] ] ]
        arrow_policy.append(arr)

arrow_policy = np.array(arrow_policy).reshape(4,4)

print("Optimal Policy (arrows):")
print(arrow_policy)

print("\nQ-table:")
print(Q)

    </pre>

    <pre id="function">
        #Q13: To implement function approximation using linear model

import numpy as np

# Gridworld
N_ROWS, N_COLS = 4, 4
TERMINAL = [0, 15]

# Step function
def step(s, a):
    r, c = divmod(s, N_COLS)
    dr, dc = {0:(-1,0), 1:(1,0), 2:(0,-1), 3:(0,1)}[a]  # U,D,L,R
    nr, nc = min(max(r + dr, 0), N_ROWS-1), min(max(c + dc, 0), N_COLS-1)
    ns = nr * N_COLS + nc
    reward = -1
    return ns, reward

# --- Features ---
def features(s):
    r, c = divmod(s, N_COLS)
    return np.array([r, c, 1.0])   # simple 3-dimensional features

# --- Linear Value Function ---
def V(s, w):
    return np.dot(w, features(s))

# --- TD(0) with function approximation ---
def td_linear(alpha=0.01, gamma=0.99, episodes=1000):
    w = np.zeros(3)  # weights for the linear model

    for ep in range(episodes):
        s = np.random.randint(1, 15)   # start non-terminal

        while s not in TERMINAL:
            # choose a random action for prediction-only
            a = np.random.choice([0,1,2,3])

            ns, r = step(s, a)

            # TD target
            td_target = r + gamma * (0 if ns in TERMINAL else V(ns, w))
            td_error = td_target - V(s, w)

            # gradient update: w += alpha * error * x(s)
            w += alpha * td_error * features(s)

            s = ns

    return w

# --- Run ---
w = td_linear()
print("Learned weights w:", w)

# Value table from approximator
value_table = np.array([V(s, w) for s in range(16)]).reshape(4,4)
print("\nApproximated Value Function:")
print(value_table)

    </pre>


    <pre id="keywords">
        Policy evaluation using Monte Carlo Method (using any method for 4×4 grid world)=policy
        Simple MDP simulation in a 4×4 grid world for Markov Decision Process (MDP) for planning with Value Iteration=mdp
        Simple MDP simulation in a 4x4 grid world for markov decision process (MDP) for Rollout the optimal policy from (0,0)=mdprollout
        Simple MDP simulation in a 4x4 grid world for Markov Decision Process (MDP), 
        Learn from interaction with Q-learning (unknown model)= mdpunknown
        TD(0)= td0
        #TD(lambda)Learning= td1
        Implement SARSA= sarsa
        Q Learning= qlearning
        DQN=dqn
        Implement REINFORCE algorithm with Baseline= reinforce
        Implement REINFORCE algorithm with Advantage Function.=reinforce1
        Implement the Monte Carlo prediction and control=control
        To implement function approximation using linear model=function

    </pre>


    
   

    
</body>
</html>
